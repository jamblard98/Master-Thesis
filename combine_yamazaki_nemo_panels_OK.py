#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
combine_yamazaki_nemo_panels_CLEAN.py (LON 0-360¬∞)

Panneau 4√ó2 optimis√© : Yamazaki (gauche) vs NEMO (droite)
Profils d'anomalie par mer.

LOGIQUE LONGITUDE 0-360¬∞ (sens horaire) :
- Ross: 160¬∞ ‚Üí 210¬∞ (50¬∞)
- Weddell: 300¬∞ ‚Üí 340¬∞ (40¬∞)
- Bellingshausen-Amundsen: 230¬∞ ‚Üí 300¬∞ (70¬∞)
- Davis: 80¬∞ ‚Üí 100¬∞ (20¬∞)
"""

import argparse
import glob
from pathlib import Path
import numpy as np
import pandas as pd

# IMPORTANT: backend non-interactif pour √©viter les erreurs Qt/Wayland en environnement headless
import matplotlib
matplotlib.use("Agg")

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from matplotlib.lines import Line2D
from matplotlib.transforms import Bbox

# === AJOUTS (Excel + styles) ===
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side
from openpyxl.worksheet.worksheet import Worksheet


SEASON_MONTHS = {
    'DJF': [12, 1, 2],
    'MAM': [3, 4, 5],
    'JJA': [6, 7, 8],
    'SON': [9, 10, 11],
    'ALL': list(range(1, 13)),
}

# =============================================================================
# AJOUT : BANDES DE LATITUDE (pour profils latitudinaux) ‚Äî coh√©rent avec la figure jointe
# =============================================================================

LAT_BANDS = [
    ("60-70¬∞S", -70.0, -60.0),  # [-70, -60[
    ("70-80¬∞S", -80.0, -70.0),  # [-80, -70[
]

# =============================================================================
# D√âFINITION DES MERS (0-360¬∞ sens horaire)
# =============================================================================

SEAS = {
    'Ross': {
        'lat_min': -78, 'lat_max': -70,
        'lon_start': 160,   # 160¬∞E
        'lon_end': 210,     # 210¬∞ = -150¬∞W
        'label': 'Ross'
    },
    'Weddell': {
        'lat_min': -78, 'lat_max': -60,
        'lon_start': 300,   # 300¬∞ = -60¬∞W
        'lon_end': 340,     # 340¬∞ = -20¬∞W
        'label': 'Weddell'
    },
    'Bellingshausen-Amundsen': {
        'lat_min': -75, 'lat_max': -65,
        'lon_start': 230,   # 230¬∞ = -130¬∞W
        'lon_end': 300,     # 300¬∞ = -60¬∞W
        'label': 'Bellingshausen/\nAmundsen'
    },
    'Davis': {
        'lat_min': -68, 'lat_max': -65,
        'lon_start': 80,    # 80¬∞E
        'lon_end': 100,     # 100¬∞E
        'label': 'Davis'
    }
}

SEA_ORDER = ['Bellingshausen-Amundsen', 'Weddell', 'Davis', 'Ross']

DECADES = [
    (1900, 1910), (1910, 1920), (1920, 1930), (1930, 1940),
    (1940, 1950), (1950, 1960), (1960, 1970)
]

DECADE_COLORS = ['#1f77b4', '#ff7f0e', '#2ca02c',
                 '#d62728', '#9467bd', '#8c564b', '#e377c2']

# =============================================================================
# FONCTIONS UTILITAIRES
# =============================================================================

def _safe_read_csvs(pattern: str) -> pd.DataFrame:
    files = sorted(glob.glob(pattern))

    print(f"üîç Pattern: {pattern}")
    print(f"üîç Fichiers trouv√©s: {len(files)}")

    dfs = []
    total_lines = 0

    for f in files:
        try:
            df = pd.read_csv(f)
            if df.dropna(how='all').empty:
                continue
            dfs.append(df)
            fname = Path(f).name
            print(f"   ‚úì {fname}: {len(df):,} lignes")
            total_lines += len(df)
        except Exception as e:
            print(f"‚ö†Ô∏è  Lecture √©chou√©e: {f} ({e})")

    print(f"üìä TOTAL charg√©: {total_lines:,} lignes depuis {len(dfs)} fichiers")

    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()


def normalize_longitude_360(lon):
    """Convertit longitude [-180,180] vers [0,360]."""
    return lon % 360


def filter_by_sea(df, sea_config):
    """
    Filtre DataFrame par mer avec logique 0-360¬∞.

    Args:
        df: DataFrame avec 'hist_lat' et 'hist_lon' (en [-180,180])
        sea_config: Dict avec lat_min, lat_max, lon_start, lon_end

    Returns:
        DataFrame filtr√©
    """
    # Convertir longitudes en 0-360
    df = df.copy()
    df['lon_360'] = normalize_longitude_360(df['hist_lon'])

    # Filtrer latitude
    df_sea = df[
        (df['hist_lat'] >= sea_config['lat_min']) &
        (df['hist_lat'] < sea_config['lat_max'])
    ]

    # Filtrer longitude (sens horaire)
    lon_start = sea_config['lon_start']
    lon_end = sea_config['lon_end']

    if lon_start <= lon_end:
        # Arc normal (ex: Davis 80‚Üí100)
        df_sea = df_sea[
            (df_sea['lon_360'] >= lon_start) &
            (df_sea['lon_360'] < lon_end)
        ]
    else:
        # Passage par 0¬∞ (ne devrait pas arriver avec notre config)
        df_sea = df_sea[
            (df_sea['lon_360'] >= lon_start) |
            (df_sea['lon_360'] < lon_end)
        ]

    return df_sea


# =============================================================================
# AJOUTS : STATISTIQUES "FORENSICS" POUR EXPLIQUER LES POINTS ATYPIQUES (FIG. 7)
# =============================================================================

def _depth_bin_label(z, bin_size=10, zmin=10, zmax=200):
    """Retourne un label de bin (ex: '140-150') pour une profondeur z."""
    if not np.isfinite(z):
        return None
    b0 = int(np.floor((z - zmin) / bin_size) * bin_size + zmin)
    b1 = b0 + bin_size
    if b0 < zmin or b1 > (zmax + 1e-9):
        return None
    return f"{b0}-{b1}"


def _make_cell_id(df, lat_col='hist_lat', lon_col='hist_lon', res_deg=2):
    """
    Cr√©e une 'cellule' approx 2¬∞√ó2¬∞ (par d√©faut) pour diagnostiquer
    la domination spatiale (un seul endroit qui tire tout).

    NOTE: ajoute aussi explicitement 'lon_360' pour √©viter tout KeyError
    apr√®s des groupby (o√π lon_360 peut dispara√Ætre).
    """
    d = df.copy()
    d['cell_lat'] = (np.floor(d[lat_col] / res_deg) * res_deg).astype(int)

    lon360 = normalize_longitude_360(d[lon_col].to_numpy())
    d['lon_360'] = lon360  # <-- FIX: garantir la pr√©sence de lon_360
    d['cell_lon'] = (np.floor(lon360 / res_deg) * res_deg).astype(int)

    d['cell_id'] = d['cell_lat'].astype(str) + "_" + d['cell_lon'].astype(str)
    return d


def _robust_stats(x):
    """Stats robustes + classiques, x array-like."""
    x = np.asarray(x)
    x = x[np.isfinite(x)]
    if x.size == 0:
        return dict(N=0, mean=np.nan, std=np.nan, sem=np.nan,
                    median=np.nan, p10=np.nan, p90=np.nan, iqr=np.nan)
    mean = float(np.mean(x))
    std = float(np.std(x, ddof=1)) if x.size >= 2 else np.nan
    sem = float(std / np.sqrt(x.size)) if np.isfinite(std) else np.nan
    q10, q25, q50, q75, q90 = np.percentile(x, [10, 25, 50, 75, 90])
    return dict(
        N=int(x.size),
        mean=mean,
        std=std,
        sem=sem,
        median=float(q50),
        p10=float(q10),
        p90=float(q90),
        iqr=float(q75 - q25)
    )


def _bootstrap_ci_mean(x, n_boot=500, ci=95, seed=0):
    """IC bootstrap sur la moyenne. Renvoie (low, high)."""
    x = np.asarray(x)
    x = x[np.isfinite(x)]
    if x.size < 3:
        return (np.nan, np.nan)
    rng = np.random.default_rng(seed)
    boots = rng.choice(x, size=(n_boot, x.size), replace=True).mean(axis=1)
    alpha = (100 - ci) / 2
    lo, hi = np.percentile(boots, [alpha, 100 - alpha])
    return (float(lo), float(hi))


def _compute_profile_forensics_tables(
    sea_name, df_sea_yam, df_sea_nem, out_dir: Path,
    depth_min=10, depth_max=200, bin_size=10,
    cell_res_deg=2, do_bootstrap=True,
    silent=False
):
    """
    Produit des tables CSV + prints terminal pour diagnostiquer :
    - si certains "points" d'un profil sont domin√©s par 1-2 obs / 1 ann√©e / 1 cellule
    - la distribution (moyenne, m√©diane, p10/p90, iqr, etc.) par bin de profondeur
    - le biais NEMO - OBS par bin (moyenne & m√©diane)
    - les outliers (top 10 chaud/froid) avec date + lat/lon + profondeur + cellule
    """
    # --------------------
    # Pr√©parer OBS (observations - Yamazaki)
    # --------------------
    obs = df_sea_yam.copy()
    obs = obs[(obs['hist_depth_m'] >= depth_min) & (obs['hist_depth_m'] <= depth_max)]
    if obs.empty:
        if not silent:
            print(f"   ‚ö†Ô∏è  Forensics: pas de donn√©es OBS pour {sea_name}")
        return

    obs['delta_T'] = obs['hist_temperature'] - obs['yamazaki_T']
    obs['depth_bin'] = obs['hist_depth_m'].apply(lambda z: _depth_bin_label(z, bin_size, depth_min, depth_max))
    obs = obs.dropna(subset=['depth_bin'])
    obs = _make_cell_id(obs, 'hist_lat', 'hist_lon', res_deg=cell_res_deg)

    # --------------------
    # Pr√©parer NEMO (hist - recent_mean)
    # --------------------
    mod = df_sea_nem.copy()
    mod = mod[(mod['hist_depth_m'] >= depth_min) & (mod['hist_depth_m'] <= depth_max)]
    if mod.empty:
        if not silent:
            print(f"   ‚ö†Ô∏è  Forensics: pas de donn√©es NEMO pour {sea_name}")
        return

    group_cols = ['hist_year', 'hist_month', 'hist_day',
                  'hist_lat', 'hist_lon', 'hist_depth_m', 'nemo_hist_T']
    modg = mod.groupby(group_cols, as_index=False).agg({'nemo_recent_T': 'mean'})
    modg.rename(columns={'nemo_recent_T': 'nemo_recent_mean'}, inplace=True)
    modg['delta_T'] = modg['nemo_hist_T'] - modg['nemo_recent_mean']

    modg['depth_bin'] = modg['hist_depth_m'].apply(lambda z: _depth_bin_label(z, bin_size, depth_min, depth_max))
    modg = modg.dropna(subset=['depth_bin'])
    modg = _make_cell_id(modg, 'hist_lat', 'hist_lon', res_deg=cell_res_deg)

    # --------------------
    # Agr√©gation par d√©cennie & bin
    # --------------------
    rows_obs = []
    rows_mod = []
    rows_outliers = []

    def _collect_outliers(df, dataset_tag, dec_label):
        if df.empty:
            return
        cols = ['hist_year', 'hist_month', 'hist_day',
                'hist_lat', 'hist_lon', 'lon_360',
                'hist_depth_m', 'depth_bin', 'cell_id', 'delta_T']
        if 'lon_360' not in df.columns:
            df = df.copy()
            df['lon_360'] = normalize_longitude_360(df['hist_lon'])
        d = df[cols].copy()
        d = d[np.isfinite(d['delta_T'])]
        if d.empty:
            return
        top_hot = d.nlargest(10, 'delta_T')
        top_cold = d.nsmallest(10, 'delta_T')
        for kind, dd in [('hot', top_hot), ('cold', top_cold)]:
            for _, r in dd.iterrows():
                rows_outliers.append({
                    'Sea': sea_name,
                    'Dataset': dataset_tag,
                    'Decade': dec_label,
                    'Kind': kind,
                    'year': int(r['hist_year']),
                    'month': int(r['hist_month']),
                    'day': int(r['hist_day']),
                    'lat': float(r['hist_lat']),
                    'lon': float(r['hist_lon']),
                    'lon_360': float(r['lon_360']) if np.isfinite(r['lon_360']) else np.nan,
                    'depth_m': float(r['hist_depth_m']),
                    'depth_bin': r['depth_bin'],
                    'cell_id': r['cell_id'],
                    'delta_T': float(r['delta_T'])
                })

    def _summarize_bins(df, dataset_tag, dec_label, rows):
        if df.empty:
            return
        for depth_bin, sub in df.groupby('depth_bin'):
            x = sub['delta_T'].to_numpy()
            st = _robust_stats(x)

            ci_lo, ci_hi = (np.nan, np.nan)
            if do_bootstrap and st['N'] >= 10:
                ci_lo, ci_hi = _bootstrap_ci_mean(x, n_boot=500, ci=95, seed=0)

            n_years = int(sub['hist_year'].nunique())
            year_counts = sub['hist_year'].value_counts()
            top_year_frac = float(year_counts.iloc[0] / st['N']) if st['N'] > 0 else np.nan

            n_cells = int(sub['cell_id'].nunique())
            cell_counts = sub['cell_id'].value_counts()
            top_cell_frac = float(cell_counts.iloc[0] / st['N']) if st['N'] > 0 else np.nan

            rows.append({
                'Sea': sea_name,
                'Dataset': dataset_tag,
                'Decade': dec_label,
                'depth_bin': depth_bin,
                **st,
                'ci95_lo': ci_lo,
                'ci95_hi': ci_hi,
                'n_unique_years': n_years,
                'top_year_frac': top_year_frac,
                'n_unique_cells': n_cells,
                'top_cell_frac': top_cell_frac,
            })

    for (dec0, dec1) in DECADES:
        dec_label = f"{dec0}-{dec1-1}"
        obs_dec = obs[(obs['hist_year'] >= dec0) & (obs['hist_year'] < dec1)]
        mod_dec = modg[(modg['hist_year'] >= dec0) & (modg['hist_year'] < dec1)]

        _collect_outliers(obs_dec, "OBS", dec_label)
        _collect_outliers(mod_dec, "NEMO", dec_label)

        _summarize_bins(obs_dec, "OBS", dec_label, rows_obs)
        _summarize_bins(mod_dec, "NEMO", dec_label, rows_mod)

    df_obs_stats = pd.DataFrame(rows_obs).sort_values(['Sea', 'Decade', 'depth_bin'])
    df_mod_stats = pd.DataFrame(rows_mod).sort_values(['Sea', 'Decade', 'depth_bin'])
    df_out = pd.DataFrame(rows_outliers)

    # --------------------
    # Biais NEMO - OBS par bin (jointure)
    # --------------------
    if not df_obs_stats.empty and not df_mod_stats.empty:
        j = df_obs_stats.merge(
            df_mod_stats,
            on=['Sea', 'Decade', 'depth_bin'],
            suffixes=('_obs', '_mod'),
            how='inner'
        )
        j['bias_mean_mod_minus_obs'] = j['mean_mod'] - j['mean_obs']
        j['bias_median_mod_minus_obs'] = j['median_mod'] - j['median_obs']
    else:
        j = pd.DataFrame()

    # --------------------
    # Sauvegarde CSV
    # --------------------
    sea_tag = SEAS[sea_name]['label'].replace('\n', '').replace('/', '-').replace(' ', '')
    out_obs = out_dir / f"forensics_obs_{sea_tag}.csv"
    out_mod = out_dir / f"forensics_nemo_{sea_tag}.csv"
    out_bias = out_dir / f"forensics_bias_nemo_minus_obs_{sea_tag}.csv"
    out_ol  = out_dir / f"forensics_outliers_{sea_tag}.csv"

    df_obs_stats.to_csv(out_obs, index=False)
    df_mod_stats.to_csv(out_mod, index=False)
    if not j.empty:
        j.to_csv(out_bias, index=False)
    if not df_out.empty:
        df_out.to_csv(out_ol, index=False)

    if silent:
        return

    # --------------------
    # PRINTS TERMINAL (lisible)
    # --------------------
    print("\n" + "="*92)
    print(f"üìå FORENSICS FIG.7 ‚Äî {sea_name}  |  cell_res={cell_res_deg}¬∞  |  bins={bin_size}m  |  depth={depth_min}-{depth_max}m")
    print("="*92)

    def _print_summary(df_stats, title):
        if df_stats.empty:
            print(f"‚ö†Ô∏è  {title}: aucun r√©sultat.")
            return
        cols = ['Decade', 'depth_bin', 'N', 'mean', 'median', 'p10', 'p90', 'std', 'sem',
                'ci95_lo', 'ci95_hi', 'n_unique_years', 'top_year_frac', 'n_unique_cells', 'top_cell_frac']
        d = df_stats.copy()
        for c in ['mean', 'median', 'p10', 'p90', 'std', 'sem', 'ci95_lo', 'ci95_hi', 'top_year_frac', 'top_cell_frac']:
            if c in d.columns:
                d[c] = pd.to_numeric(d[c], errors='coerce')
        for dec in d['Decade'].unique():
            dd = d[d['Decade'] == dec][cols].copy()
            dd = dd.sort_values('depth_bin')
            print(f"\n--- {title} | {dec} ---")
            with pd.option_context('display.max_rows', 500, 'display.width', 220):
                print(dd.to_string(index=False, float_format=lambda x: f"{x:,.3f}"))

    _print_summary(df_obs_stats[df_obs_stats['Dataset'] == 'OBS'], "OBS (obs - Yamazaki)")
    _print_summary(df_mod_stats[df_mod_stats['Dataset'] == 'NEMO'], "NEMO (hist - recent_mean)")

    if not j.empty:
        print("\n" + "-"*92)
        print("BIAIS NEMO - OBS (par bin de profondeur) :")
        print("-"*92)
        cols_b = ['Decade', 'depth_bin', 'N_obs', 'N_mod', 'mean_obs', 'mean_mod',
                  'bias_mean_mod_minus_obs', 'median_obs', 'median_mod', 'bias_median_mod_minus_obs']
        jb = j[cols_b].copy().sort_values(['Decade', 'depth_bin'])
        with pd.option_context('display.max_rows', 500, 'display.width', 220):
            print(jb.to_string(index=False, float_format=lambda x: f"{x:,.3f}"))

    if not df_out.empty:
        print("\n" + "-"*92)
        print("OUTLIERS (top10 chaud/froid) ‚Äî utile pour rep√©rer '1 ann√©e' ou '1 cellule' :")
        print("-"*92)
        for dec in df_out['Decade'].unique():
            ddec = df_out[df_out['Decade'] == dec]
            hot = ddec[ddec['Kind'] == 'hot'].sort_values('delta_T', ascending=False).head(5)
            cold = ddec[ddec['Kind'] == 'cold'].sort_values('delta_T', ascending=True).head(5)
            print(f"\n--- Outliers {sea_name} | {dec} | HOT (top 5) ---")
            with pd.option_context('display.width', 220):
                print(hot[['Dataset','year','month','day','lat','lon_360','depth_m','depth_bin','cell_id','delta_T']].to_string(index=False, float_format=lambda x: f"{x:,.3f}"))
            print(f"\n--- Outliers {sea_name} | {dec} | COLD (top 5) ---")
            with pd.option_context('display.width', 220):
                print(cold[['Dataset','year','month','day','lat','lon_360','depth_m','depth_bin','cell_id','delta_T']].to_string(index=False, float_format=lambda x: f"{x:,.3f}"))

    print("\n‚úÖ CSV g√©n√©r√©s :")
    print(f"   - {out_obs.name}")
    print(f"   - {out_mod.name}")
    if not j.empty:
        print(f"   - {out_bias.name}")
    if not df_out.empty:
        print(f"   - {out_ol.name}")
    print("="*92 + "\n")


# =============================================================================
# FONCTIONS DE TRAC√â
# =============================================================================

def _plot_yamazaki_sea_on_ax(ax, df_sea, sea_name, season):
    """Dessine profils d'anomalie Yamazaki pour une mer."""
    has_data = False

    for idx, (dec0, dec1) in enumerate(DECADES):
        df_dec = df_sea[(df_sea['hist_year'] >= dec0) &
                        (df_sea['hist_year'] < dec1)]
        if len(df_dec) == 0:
            continue

        has_data = True

        zmin = max(10, int(np.floor(df_dec['hist_depth_m'].min())))
        zmax = min(200, int(np.ceil(df_dec['hist_depth_m'].max())))
        depth_bins = np.arange(zmin, zmax + 10, 10)
        if len(depth_bins) < 2:
            continue
        depth_centers = (depth_bins[:-1] + depth_bins[1:]) / 2

        anom_mean = []
        for i in range(len(depth_bins) - 1):
            m = ((df_dec['hist_depth_m'] >= depth_bins[i]) &
                 (df_dec['hist_depth_m'] < depth_bins[i + 1]))
            sub = df_dec[m]
            if len(sub) > 0:
                anom_mean.append(
                    (sub['hist_temperature'] - sub['yamazaki_T']).mean()
                )
            else:
                anom_mean.append(np.nan)

        anom_mean = np.array(anom_mean)
        valid = ~np.isnan(anom_mean)

        if valid.any():
            ax.plot(anom_mean[valid], depth_centers[valid], '-',
                    color=DECADE_COLORS[idx], linewidth=2,
                    marker='o', markersize=4, alpha=0.9)

    if not has_data:
        ax.text(0.5, 0.5, "Pas de donn√©es", transform=ax.transAxes,
                ha='center', va='center', fontsize=9, color='gray')

    ax.set_xlim(-2.5, 2.5)
    ax.set_ylim(200, 10)
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.axvline(0, color='black', linewidth=0.8, alpha=0.4)


def _plot_nemo_sea_on_ax(ax, df_sea, sea_name, season):
    """Dessine profils d'anomalie NEMO pour une mer."""
    has_data = False

    for idx, (dec0, dec1) in enumerate(DECADES):
        df_dec = df_sea[(df_sea['hist_year'] >= dec0) &
                        (df_sea['hist_year'] < dec1)]
        if len(df_dec) == 0:
            continue

        # Grouper et moyenner ann√©es r√©centes
        group_cols = ['hist_year', 'hist_month', 'hist_day',
                      'hist_lat', 'hist_lon', 'hist_depth_m', 'nemo_hist_T']

        df_grouped = df_dec.groupby(group_cols, as_index=False).agg({
            'nemo_recent_T': 'mean'
        })
        df_grouped.rename(columns={'nemo_recent_T': 'nemo_recent_mean'},
                          inplace=True)
        df_grouped['delta_T'] = (
            df_grouped['nemo_hist_T'] - df_grouped['nemo_recent_mean']
        )

        has_data = True

        zmin = max(10, int(np.floor(df_grouped['hist_depth_m'].min())))
        zmax = min(200, int(np.ceil(df_grouped['hist_depth_m'].max())))
        depth_bins = np.arange(zmin, zmax + 10, 10)
        if len(depth_bins) < 2:
            continue
        depth_centers = (depth_bins[:-1] + depth_bins[1:]) / 2

        anom_mean = []
        for i in range(len(depth_bins) - 1):
            m = ((df_grouped['hist_depth_m'] >= depth_bins[i]) &
                 (df_grouped['hist_depth_m'] < depth_bins[i + 1]))
            sub = df_grouped[m]
            if len(sub) > 0:
                anom_mean.append(sub['delta_T'].mean())
            else:
                anom_mean.append(np.nan)

        anom_mean = np.array(anom_mean)
        valid = ~np.isnan(anom_mean)

        if valid.any():
            ax.plot(anom_mean[valid], depth_centers[valid], '-',
                    color=DECADE_COLORS[idx], linewidth=2,
                    marker='o', markersize=4, alpha=0.9)

    if not has_data:
        ax.text(0.5, 0.5, "Pas de donn√©es", transform=ax.transAxes,
                ha='center', va='center', fontsize=9, color='gray')

    ax.set_xlim(-2.5, 2.5)
    ax.set_ylim(200, 10)
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.axvline(0, color='black', linewidth=0.8, alpha=0.4)


# =============================================================================
# PROFILS MOYENS & STATISTIQUES VERTICALES (TABLEAU)  [conserv√© mais optionnel]
# =============================================================================

def _compute_yamazaki_profile(df_sea, year0, year1,
                              depth_min=10, depth_max=200,
                              bin_size=10):
    """Profil moyen ŒîT_obs(z) = obs - Yamazaki sur [year0, year1)."""
    df_dec = df_sea[
        (df_sea['hist_year'] >= year0) &
        (df_sea['hist_year'] < year1)
    ].copy()
    if df_dec.empty:
        return np.array([]), np.array([])

    df_dec = df_dec[
        (df_dec['hist_depth_m'] >= depth_min) &
        (df_dec['hist_depth_m'] <= depth_max)
    ]
    if df_dec.empty:
        return np.array([]), np.array([])

    zmin = max(depth_min, int(np.floor(df_dec['hist_depth_m'].min())))
    zmax = min(depth_max, int(np.ceil(df_dec['hist_depth_m'].max())))
    if zmax <= zmin:
        return np.array([]), np.array([])

    depth_bins = np.arange(zmin, zmax + bin_size, bin_size)
    if len(depth_bins) < 2:
        return np.array([]), np.array([])

    depth_centers = (depth_bins[:-1] + depth_bins[1:]) / 2
    anom_mean = []

    for i in range(len(depth_bins) - 1):
        m = ((df_dec['hist_depth_m'] >= depth_bins[i]) &
             (df_dec['hist_depth_m'] < depth_bins[i + 1]))
        sub = df_dec[m]
        if len(sub) > 0:
            vals = sub['hist_temperature'] - sub['yamazaki_T']
            anom_mean.append(vals.mean())
        else:
            anom_mean.append(np.nan)

    return depth_centers, np.array(anom_mean)


def _compute_nemo_profile(df_sea, year0, year1,
                          depth_min=10, depth_max=200,
                          bin_size=10):
    """Profil moyen ŒîT_NEMO(z) = NEMO_hist - NEMO_recent_mean sur [year0, year1)."""
    df_dec = df_sea[
        (df_sea['hist_year'] >= year0) &
        (df_sea['hist_year'] < year1)
    ].copy()
    if df_dec.empty:
        return np.array([]), np.array([])

    group_cols = ['hist_year', 'hist_month', 'hist_day',
                  'hist_lat', 'hist_lon', 'hist_depth_m', 'nemo_hist_T']
    df_grouped = df_dec.groupby(group_cols, as_index=False).agg({
        'nemo_recent_T': 'mean'
    })
    df_grouped.rename(columns={'nemo_recent_T': 'nemo_recent_mean'},
                      inplace=True)
    df_grouped['delta_T'] = (
        df_grouped['nemo_hist_T'] - df_grouped['nemo_recent_mean']
    )

    df_grouped = df_grouped[
        (df_grouped['hist_depth_m'] >= depth_min) &
        (df_grouped['hist_depth_m'] <= depth_max)
    ]
    if df_grouped.empty:
        return np.array([]), np.array([])

    zmin = max(depth_min, int(np.floor(df_grouped['hist_depth_m'].min())))
    zmax = min(depth_max, int(np.ceil(df_grouped['hist_depth_m'].max())))
    if zmax <= zmin:
        return np.array([]), np.array([])

    depth_bins = np.arange(zmin, zmax + bin_size, bin_size)
    if len(depth_bins) < 2:
        return np.array([]), np.array([])

    depth_centers = (depth_bins[:-1] + depth_bins[1:]) / 2
    anom_mean = []

    for i in range(len(depth_bins) - 1):
        m = ((df_grouped['hist_depth_m'] >= depth_bins[i]) &
             (df_grouped['hist_depth_m'] < depth_bins[i + 1]))
        sub = df_grouped[m]
        if len(sub) > 0:
            anom_mean.append(sub['delta_T'].mean())
        else:
            anom_mean.append(np.nan)

    return depth_centers, np.array(anom_mean)


def _align_profiles(depths1, vals1, depths2, vals2):
    """Aligne deux profils sur les profondeurs communes et vire les NaN."""
    if len(depths1) == 0 or len(depths2) == 0:
        return np.array([]), np.array([])

    df1 = pd.DataFrame({'depth': depths1, 'v1': vals1})
    df2 = pd.DataFrame({'depth': depths2, 'v2': vals2})
    dfm = df1.merge(df2, on='depth', how='inner')
    if dfm.empty:
        return np.array([]), np.array([])

    mask = np.isfinite(dfm['v1']) & np.isfinite(dfm['v2'])
    dfm = dfm[mask]
    if dfm.empty:
        return np.array([]), np.array([])

    return dfm['v1'].to_numpy(), dfm['v2'].to_numpy()


def _compute_vertical_stats_for_sea(sea_name,
                                    df_sea_yam,
                                    df_sea_nem,
                                    depth_min=10,
                                    depth_max=200):
    """
    Calcule les stats verticales pour une mer :
    - r_1900_1929, r_1930_1969
    - RMSE_1900_1969
    - amplitude_obs, amplitude_nemo
    - N_obs, N_mod
    """
    d_obs_1_z, d_obs_1_v = _compute_yamazaki_profile(
        df_sea_yam, 1900, 1930, depth_min, depth_max
    )
    d_mod_1_z, d_mod_1_v = _compute_nemo_profile(
        df_sea_nem, 1900, 1930, depth_min, depth_max
    )
    v1_obs, v1_mod = _align_profiles(d_obs_1_z, d_obs_1_v,
                                     d_mod_1_z, d_mod_1_v)
    if len(v1_obs) > 0:
        r_1900_1929 = float(np.corrcoef(v1_obs, v1_mod)[0, 1])
    else:
        r_1900_1929 = np.nan

    d_obs_2_z, d_obs_2_v = _compute_yamazaki_profile(
        df_sea_yam, 1930, 1970, depth_min, depth_max
    )
    d_mod_2_z, d_mod_2_v = _compute_nemo_profile(
        df_sea_nem, 1930, 1970, depth_min, depth_max
    )
    v2_obs, v2_mod = _align_profiles(d_obs_2_z, d_obs_2_v,
                                     d_mod_2_z, d_mod_2_v)
    if len(v2_obs) > 0:
        r_1930_1969 = float(np.corrcoef(v2_obs, v2_mod)[0, 1])
    else:
        r_1930_1969 = np.nan

    d_obs_all_z, d_obs_all_v = _compute_yamazaki_profile(
        df_sea_yam, 1900, 1970, depth_min, depth_max
    )
    d_mod_all_z, d_mod_all_v = _compute_nemo_profile(
        df_sea_nem, 1900, 1970, depth_min, depth_max
    )

    N_obs = int(np.isfinite(d_obs_all_v).sum()) if len(d_obs_all_v) > 0 else 0
    N_mod = int(np.isfinite(d_mod_all_v).sum()) if len(d_mod_all_v) > 0 else 0

    v_all_obs, v_all_mod = _align_profiles(d_obs_all_z, d_obs_all_v,
                                           d_mod_all_z, d_mod_all_v)
    if len(v_all_obs) > 0:
        diff = v_all_obs - v_all_mod
        RMSE_1900_1969 = float(np.sqrt(np.mean(diff ** 2)))
    else:
        RMSE_1900_1969 = np.nan

    if N_obs > 0:
        amplitude_obs = float(np.nanmax(d_obs_all_v) - np.nanmin(d_obs_all_v))
    else:
        amplitude_obs = np.nan

    if N_mod > 0:
        amplitude_nemo = float(np.nanmax(d_mod_all_v) - np.nanmin(d_mod_all_v))
    else:
        amplitude_nemo = np.nan

    sea_label = SEAS[sea_name]['label'].replace('\n', ' ')

    return {
        'Sea': sea_label,
        'r_1900_1929': r_1900_1929,
        'r_1930_1969': r_1930_1969,
        'RMSE_1900_1969': RMSE_1900_1969,
        'amplitude_obs': amplitude_obs,
        'amplitude_nemo': amplitude_nemo,
        'N_obs': N_obs,
        'N_mod': N_mod
    }


def _save_vertical_stats_table(stats_rows, out_dir: Path, silent=False):
    """Sauvegarde le tableau de stats verticales en CSV + print terminal."""
    if not stats_rows:
        if not silent:
            print("\n‚ö†Ô∏è  Aucune statistique verticale √† sauvegarder.")
        return

    df_stats = pd.DataFrame(stats_rows)

    cols = [
        'Sea',
        'r_1900_1929',
        'r_1930_1969',
        'RMSE_1900_1969',
        'amplitude_obs',
        'amplitude_nemo',
        'N_obs',
        'N_mod'
    ]
    df_stats = df_stats[cols]

    out_csv = out_dir / "NEMO_Yamazaki_comparison.csv"
    df_stats.to_csv(out_csv, index=False)

    if not silent:
        print("\n" + "="*72)
        print("üìä TABLEAU DE COMPARAISON NEMO vs YAMAZAKI (statistiques verticales)")
        print("="*72)
        pd.options.display.float_format = "{:,.3f}".format
        print(df_stats.to_string(index=False))
        print("="*72 + "\n")
        print(f"‚úÖ Tableau comparatif sauvegard√© : {out_csv.name}")


# =============================================================================
# AJOUTS : TABLEAUX STATS (Tableau 1 / M / D) + Excel + PNG propre
# =============================================================================

def _fmt_depth_label(d0, d1):
    # Conserver exactement tes labels + ajout shallow (10‚Äì50, 50‚Äì100)
    if abs(d0 - 10) < 1e-9 and abs(d1 - 50) < 1e-9:
        return "10‚Äì50 m"
    if abs(d0 - 50) < 1e-9 and abs(d1 - 100) < 1e-9:
        return "50‚Äì100 m"
    if abs(d0 - 10) < 1e-9 and abs(d1 - 100) < 1e-9:
        return "10‚Äì100 m"
    if abs(d0 - 100) < 1e-9 and abs(d1 - 200) < 1e-9:
        return "100‚Äì200 m"
    if abs(d0 - 10) < 1e-9 and abs(d1 - 200) < 1e-9:
        return "10‚Äì200 m"
    return f"{int(d0)}‚Äì{int(d1)} m"


def _compute_obs_delta_points(df_obs: pd.DataFrame) -> pd.DataFrame:
    """
    OBS: ŒîT = T_histo - T_r√©cente = hist_temperature - yamazaki_T
    Ici on garde le point tel quel (pas de regroupement).
    """
    d = df_obs.copy()
    d['T_histo'] = d['hist_temperature']
    d['T_recent'] = d['yamazaki_T']
    d['delta_T'] = d['T_histo'] - d['T_recent']
    return d


def _compute_nemo_delta_points(df_nem: pd.DataFrame) -> pd.DataFrame:
    """
    NEMO: ŒîT = T_histo - T_r√©cente = nemo_hist_T - mean(nemo_recent_T)
    Regroupement par "observation historique unique" (m√™mes group_cols que profils NEMO).
    """
    d = df_nem.copy()
    group_cols = ['hist_year', 'hist_month', 'hist_day',
                  'hist_lat', 'hist_lon', 'hist_depth_m', 'nemo_hist_T']
    g = d.groupby(group_cols, as_index=False).agg({'nemo_recent_T': 'mean'})
    g.rename(columns={'nemo_recent_T': 'nemo_recent_mean'}, inplace=True)
    g['T_histo'] = g['nemo_hist_T']
    g['T_recent'] = g['nemo_recent_mean']
    g['delta_T'] = g['T_histo'] - g['T_recent']
    return g


def _summ_stats_scatter(delta: pd.Series):
    """
    Stats demand√©es (sans m√©diane/iqr) :
    - ‚ü®ŒîT‚ü©
    - œÉ(ŒîT)
    - % ŒîT < 0  (au-dessus de y=x selon votre convention de signe via ŒîT)
    - % ŒîT > 0  (en-dessous)
    - N
    """
    x = delta.to_numpy(dtype=float)
    x = x[np.isfinite(x)]
    n = int(x.size)
    if n == 0:
        return (np.nan, np.nan, np.nan, np.nan, 0)
    mean = float(np.mean(x))
    std = float(np.std(x, ddof=1)) if n >= 2 else np.nan
    pct_neg = float(100.0 * np.mean(x < 0.0))
    pct_pos = float(100.0 * np.mean(x > 0.0))
    return (mean, std, pct_neg, pct_pos, n)


def _build_table1(dataset_name: str, df_points: pd.DataFrame) -> pd.DataFrame:
    """
    Tableau 1 : Mer (1√®re colonne) + P√©riode fusionn√©e + Profondeur (3 niveaux),
    colonnes stats: ‚ü®ŒîT‚ü©, œÉ(ŒîT), %ŒîT<0, N.
    """
    periods = [(1900, 1930, "1900‚Äì1929"),
               (1930, 1970, "1930‚Äì1969"),
               (1900, 1970, "1900‚Äì1969")]
    depths = [(10, 100), (100, 200), (10, 200)]

    rows = []
    for sea_name in SEA_ORDER:
        sea_label = SEAS[sea_name]['label'].replace('\n', ' ')
        sea_conf = SEAS[sea_name]

        df_sea = filter_by_sea(df_points, sea_conf)

        for (y0, y1, plab) in periods:
            df_p = df_sea[(df_sea['hist_year'] >= y0) & (df_sea['hist_year'] < y1)]
            for (d0, d1) in depths:
                df_pd = df_p[(df_p['hist_depth_m'] >= d0) & (df_p['hist_depth_m'] <= d1)]
                mean, std, pct_neg, _, n = _summ_stats_scatter(df_pd['delta_T']) if len(df_pd) else (np.nan, np.nan, np.nan, np.nan, 0)
                rows.append({
                    'Mer': sea_label,
                    'P√©riode': plab,
                    'Profondeur': _fmt_depth_label(d0, d1),
                    '‚ü®ŒîT‚ü©': mean,
                    'œÉ(ŒîT)': std,
                    '% ŒîT < 0': pct_neg,
                    'N': n
                })

    return pd.DataFrame(rows)


def _build_tableM(dataset_name: str, df_points: pd.DataFrame) -> pd.DataFrame:
    """
    Tableau M (annexe) ‚Äî Statistiques sectorielles compl√®tes
    Dimensions : Mer, P√©riode, Profondeur
    Colonnes : ‚ü®ŒîT‚ü©, œÉ(ŒîT), % ŒîT < 0, N
    """
    return _build_table1(dataset_name, df_points)


def _build_tableD(dataset_name: str, df_points: pd.DataFrame) -> pd.DataFrame:
    """
    Tableau D (annexe) ‚Äî √âvolution d√©cennale par mer
    Choix : une seule tranche 10‚Äì200 m
    Lignes : D√©cennies
    Colonnes : ‚ü®ŒîT‚ü©, œÉ(ŒîT), N
    """
    depth0, depth1 = 10, 200
    rows = []
    for sea_name in SEA_ORDER:
        sea_label = SEAS[sea_name]['label'].replace('\n', ' ')
        sea_conf = SEAS[sea_name]
        df_sea = filter_by_sea(df_points, sea_conf)
        df_sea = df_sea[(df_sea['hist_depth_m'] >= depth0) & (df_sea['hist_depth_m'] <= depth1)]

        for (dec0, dec1) in DECADES:
            dec_lab = f"{dec0}‚Äì{dec1-1}"
            sub = df_sea[(df_sea['hist_year'] >= dec0) & (df_sea['hist_year'] < dec1)]
            mean, std, _, _, n = _summ_stats_scatter(sub['delta_T']) if len(sub) else (np.nan, np.nan, np.nan, np.nan, 0)
            rows.append({
                'Mer': sea_label,
                'D√©cennie': dec_lab,
                '‚ü®ŒîT‚ü©': mean,
                'œÉ(ŒîT)': std,
                'N': n
            })

    return pd.DataFrame(rows)


# =============================================================================
# AJOUT : TABLEAU "LAT√óDECADE" (statistiques par d√©cennie et bande de latitude)
# =============================================================================

def _build_table_latband_decade(dataset_name: str, df_points: pd.DataFrame) -> pd.DataFrame:
    """
    Stats par d√©cennie et bande de latitude (figure jointe) :
    - ‚ü®ŒîT‚ü© (anomalie moyenne)
    - œÉ(ŒîT)
    - N

    Remarque : les filtres saison/profondeur/p√©riode ont d√©j√† √©t√© appliqu√©s en amont
    (df_points construit √† partir de df_yam_filtered / df_nem_filtered).
    """
    rows = []
    for (band_label, lat_min, lat_max) in LAT_BANDS:
        sub_band = df_points[(df_points['hist_lat'] >= lat_min) & (df_points['hist_lat'] < lat_max)]
        for (dec0, dec1) in DECADES:
            dec_lab = f"{dec0}‚Äì{dec1-1}"
            sub = sub_band[(sub_band['hist_year'] >= dec0) & (sub_band['hist_year'] < dec1)]
            mean, std, _, _, n = _summ_stats_scatter(sub['delta_T']) if len(sub) else (np.nan, np.nan, np.nan, np.nan, 0)
            rows.append({
                'Bande de latitude': band_label,
                'D√©cennie': dec_lab,
                '‚ü®ŒîT‚ü©': mean,
                'œÉ(ŒîT)': std,
                'N': n
            })
    return pd.DataFrame(rows)


# =============================================================================
# AJOUT : NOUVEAU TABLEAU "LAT√óPERIODE√óPROFONDEUR" (ce que tu demandes)
# =============================================================================

def _build_table_latband_period_depth(dataset_name: str, df_points: pd.DataFrame) -> pd.DataFrame:
    """
    R√©sultats par bande de latitude POUR DES PERIODES (1900‚Äì1929, 1930‚Äì1969, 1900‚Äì1969)
    et POUR DES PROFONDEURS (10‚Äì100, 100‚Äì200, 10‚Äì200).

    Colonnes :
    - Bande de latitude
    - P√©riode
    - Profondeur
    - N
    - ‚ü®ŒîT‚ü©
    - œÉ(ŒîT)

    Remarque : df_points est d√©j√† filtr√© en saison et sur [depth_min, depth_max] + [y0,y1)
    en amont ; ici on sous-s√©lectionne √† nouveau par p√©riode et tranches de profondeur.
    """
    periods = [
        (1900, 1930, "1900‚Äì1929"),
        (1930, 1970, "1930‚Äì1969"),
        (1900, 1970, "1900‚Äì1969"),
    ]
    depths = [(10, 100), (100, 200), (10, 200)]

    rows = []
    for (band_label, lat_min, lat_max) in LAT_BANDS:
        sub_band = df_points[(df_points['hist_lat'] >= lat_min) & (df_points['hist_lat'] < lat_max)].copy()

        for (y0, y1, plab) in periods:
            sub_p = sub_band[(sub_band['hist_year'] >= y0) & (sub_band['hist_year'] < y1)]

            for (d0, d1) in depths:
                sub_pd = sub_p[(sub_p['hist_depth_m'] >= d0) & (sub_p['hist_depth_m'] <= d1)]
                if len(sub_pd) == 0:
                    mean, std, n = (np.nan, np.nan, 0)
                else:
                    x = pd.to_numeric(sub_pd['delta_T'], errors='coerce')
                    x = x[np.isfinite(x)]
                    n = int(x.size)
                    if n == 0:
                        mean, std = (np.nan, np.nan)
                    else:
                        mean = float(np.mean(x))
                        std = float(np.std(x, ddof=1)) if n >= 2 else np.nan

                rows.append({
                    'Bande de latitude': band_label,
                    'P√©riode': plab,
                    'Profondeur': _fmt_depth_label(d0, d1),
                    'N': n,
                    '‚ü®ŒîT‚ü©': mean,
                    'œÉ(ŒîT)': std,
                })

    out = pd.DataFrame(rows)
    if out.empty:
        return out
    return out.sort_values(['Bande de latitude', 'P√©riode', 'Profondeur']).reset_index(drop=True)


# =============================================================================
# AJOUTS : TABLEAUX "SCATTER SO" (pas de mers) ‚Äî OBS, NEMO, et combin√©
# =============================================================================

def _build_scatter_SO_table(dataset_name: str, df_points: pd.DataFrame, depths=None) -> pd.DataFrame:
    """
    Tableau SCATTER (Ocean Austral, sans distinction de mer) :
    P√©riodes : 1900‚Äì1929, 1930‚Äì1969, 1900‚Äì1969

    Profondeurs par d√©faut : 10‚Äì100, 100‚Äì200, 10‚Äì200
    Profondeurs custom possibles via depths=[(10,50),(50,100)] etc.

    Stats :
    - ‚ü®ŒîT‚ü©  (ŒîT = T_histo - T_r√©cente, point par point)
    - œÉ(ŒîT)
    - % ŒîT < 0  (historique plus froid => r√©chauffement)
    - % ŒîT > 0  (historique plus chaud => refroidissement)
    - N (paires)
    """
    periods = [(1900, 1930, "1900‚Äì1929"),
               (1930, 1970, "1930‚Äì1969"),
               (1900, 1970, "1900‚Äì1969")]

    if depths is None:
        depths = [(10, 100), (100, 200), (10, 200)]

    rows = []
    for (y0, y1, plab) in periods:
        df_p = df_points[(df_points['hist_year'] >= y0) & (df_points['hist_year'] < y1)]
        for (d0, d1) in depths:
            df_pd = df_p[(df_p['hist_depth_m'] >= d0) & (df_p['hist_depth_m'] <= d1)]
            mean, std, pct_neg, pct_pos, n = _summ_stats_scatter(df_pd['delta_T']) if len(df_pd) else (np.nan, np.nan, np.nan, np.nan, 0)
            rows.append({
                'Dataset': dataset_name,
                'P√©riode': plab,
                'Profondeur': _fmt_depth_label(d0, d1),
                '‚ü®ŒîT‚ü©': mean,
                'œÉ(ŒîT)': std,
                '% ŒîT < 0': pct_neg,
                '% ŒîT > 0': pct_pos,
                'N': n
            })
    return pd.DataFrame(rows)


def _apply_sheet_style(ws: Worksheet, header_row=1):
    thin = Side(style="thin", color="000000")
    border = Border(left=thin, right=thin, top=thin, bottom=thin)

    header_fill = PatternFill("solid", fgColor="F2F2F2")
    header_font = Font(bold=True)
    center = Alignment(horizontal="center", vertical="center", wrap_text=True)

    for row in ws.iter_rows(min_row=header_row, max_row=header_row):
        for cell in row:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = center
            cell.border = border

    for row in ws.iter_rows(min_row=header_row+1, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
        for cell in row:
            cell.border = border
            cell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)

    # largeurs colonnes raisonnables
    col_widths = {}
    for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
        for cell in row:
            val = "" if cell.value is None else str(cell.value)
            col_widths[cell.column_letter] = max(col_widths.get(cell.column_letter, 8), min(45, len(val) + 2))
    for col, w in col_widths.items():
        ws.column_dimensions[col].width = w

    # hauteur lignes
    for r in range(1, ws.max_row + 1):
        ws.row_dimensions[r].height = 18


def _merge_cells_for_table1(ws: Worksheet, sea_col=1, period_col=2, start_row=2):
    """
    Fusionne :
    - Mer : blocs de 9 lignes (3 p√©riodes √ó 3 profondeurs)
    - P√©riode : blocs de 3 lignes (3 profondeurs)
    """
    max_row = ws.max_row
    r = start_row
    while r <= max_row:
        sea_val = ws.cell(row=r, column=sea_col).value
        if sea_val is None:
            r += 1
            continue

        # fin du bloc Mer
        r2 = r
        while r2 <= max_row and ws.cell(row=r2, column=sea_col).value == sea_val:
            r2 += 1
        sea_end = r2 - 1

        if sea_end > r:
            ws.merge_cells(start_row=r, start_column=sea_col, end_row=sea_end, end_column=sea_col)

        # fusion des p√©riodes √† l'int√©rieur du bloc Mer
        rp = r
        while rp <= sea_end:
            per_val = ws.cell(row=rp, column=period_col).value
            if per_val is None:
                rp += 1
                continue
            rp2 = rp
            while rp2 <= sea_end and ws.cell(row=rp2, column=period_col).value == per_val:
                rp2 += 1
            per_end = rp2 - 1
            if per_end > rp:
                ws.merge_cells(start_row=rp, start_column=period_col, end_row=per_end, end_column=period_col)
            rp = rp2

        r = sea_end + 1


def _merge_cells_for_tableD(ws: Worksheet, sea_col=1, start_row=2):
    """
    Fusionne la colonne 'Mer' dans TableD (blocs par mer sur les d√©cennies).
    """
    max_row = ws.max_row
    r = start_row
    while r <= max_row:
        sea_val = ws.cell(row=r, column=sea_col).value
        if sea_val is None:
            r += 1
            continue
        r2 = r
        while r2 <= max_row and ws.cell(row=r2, column=sea_col).value == sea_val:
            r2 += 1
        end = r2 - 1
        if end > r:
            ws.merge_cells(start_row=r, start_column=sea_col, end_row=end, end_column=sea_col)
        r = end + 1


def _merge_cells_for_scatter_period(ws: Worksheet, period_col=1, start_row=2):
    """
    Fusionne la colonne 'P√©riode' (blocs de N lignes de profondeur) dans Scatter_SO_*.
    (Fonction g√©n√©rique : fonctionne aussi bien pour 3 profondeurs que pour 2.)
    """
    max_row = ws.max_row
    r = start_row
    while r <= max_row:
        v = ws.cell(row=r, column=period_col).value
        if v is None:
            r += 1
            continue
        r2 = r
        while r2 <= max_row and ws.cell(row=r2, column=period_col).value == v:
            r2 += 1
        end = r2 - 1
        if end > r:
            ws.merge_cells(start_row=r, start_column=period_col, end_row=end, end_column=period_col)
        r = end + 1


def _merge_cells_for_scatter_combined(ws: Worksheet, dataset_col=1, period_col=2, start_row=2):
    """
    Scatter_SO_Combined :
    - fusion verticale de Dataset (OBS puis NEMO)
    - ET fusion de P√©riode √† l'int√©rieur de chaque bloc Dataset
    """
    max_row = ws.max_row
    r = start_row
    while r <= max_row:
        dv = ws.cell(row=r, column=dataset_col).value
        if dv is None:
            r += 1
            continue

        # bloc Dataset
        r2 = r
        while r2 <= max_row and ws.cell(row=r2, column=dataset_col).value == dv:
            r2 += 1
        ds_end = r2 - 1
        if ds_end > r:
            ws.merge_cells(start_row=r, start_column=dataset_col, end_row=ds_end, end_column=dataset_col)

        # fusion P√©riode dans le bloc Dataset
        rp = r
        while rp <= ds_end:
            pv = ws.cell(row=rp, column=period_col).value
            if pv is None:
                rp += 1
                continue
            rp2 = rp
            while rp2 <= ds_end and ws.cell(row=rp2, column=period_col).value == pv:
                rp2 += 1
            pe = rp2 - 1
            if pe > rp:
                ws.merge_cells(start_row=rp, start_column=period_col, end_row=pe, end_column=period_col)
            rp = rp2

        r = ds_end + 1


# =============================================================================
# AJOUT : Fusion Excel pour "LAT√óDECADE"
# =============================================================================

def _merge_cells_for_latband_decade(ws: Worksheet, band_col=1, start_row=2):
    """
    Fusionne la colonne 'Bande de latitude' dans le tableau LAT√óDECADE.
    """
    max_row = ws.max_row
    r = start_row
    while r <= max_row:
        v = ws.cell(row=r, column=band_col).value
        if v is None:
            r += 1
            continue
        r2 = r
        while r2 <= max_row and ws.cell(row=r2, column=band_col).value == v:
            r2 += 1
        end = r2 - 1
        if end > r:
            ws.merge_cells(start_row=r, start_column=band_col, end_row=end, end_column=band_col)
        r = end + 1


# =============================================================================
# AJOUT : Fusion Excel pour "LAT√óPERIODE√óPROFONDEUR"
# =============================================================================

def _merge_cells_for_latband_period_depth(ws: Worksheet, band_col=1, period_col=2, start_row=2):
    """
    Fusionne :
    - Bande de latitude : blocs de 9 lignes (3 p√©riodes √ó 3 profondeurs)
    - P√©riode : blocs de 3 lignes (3 profondeurs) √† l'int√©rieur de chaque bande
    """
    max_row = ws.max_row
    r = start_row
    while r <= max_row:
        band_val = ws.cell(row=r, column=band_col).value
        if band_val is None:
            r += 1
            continue

        # fin du bloc bande
        r2 = r
        while r2 <= max_row and ws.cell(row=r2, column=band_col).value == band_val:
            r2 += 1
        band_end = r2 - 1

        if band_end > r:
            ws.merge_cells(start_row=r, start_column=band_col, end_row=band_end, end_column=band_col)

        # fusion des p√©riodes √† l'int√©rieur du bloc bande
        rp = r
        while rp <= band_end:
            per_val = ws.cell(row=rp, column=period_col).value
            if per_val is None:
                rp += 1
                continue
            rp2 = rp
            while rp2 <= band_end and ws.cell(row=rp2, column=period_col).value == per_val:
                rp2 += 1
            per_end = rp2 - 1
            if per_end > rp:
                ws.merge_cells(start_row=rp, start_column=period_col, end_row=per_end, end_column=period_col)
            rp = rp2

        r = band_end + 1


def _write_df_to_sheet(wb: Workbook, sheet_name: str, df: pd.DataFrame):
    if sheet_name in wb.sheetnames:
        del wb[sheet_name]
    ws = wb.create_sheet(title=sheet_name)

    for row in dataframe_to_rows(df, index=False, header=True):
        ws.append(row)

    _apply_sheet_style(ws, header_row=1)
    return ws


# =============================================================================
# PNG: fusion visuelle (texte + bordures internes supprim√©es) + format 2 d√©cimales fixes
# =============================================================================

def _cell_set_visible_edges(cell, edges: str):
    # Compatibilit√© Matplotlib (selon version)
    if hasattr(cell, "set_visible_edges"):
        cell.set_visible_edges(edges)
    else:
        cell.visible_edges = edges


def _cell_get_visible_edges(cell) -> str:
    if hasattr(cell, "get_visible_edges"):
        return cell.get_visible_edges()
    if hasattr(cell, "visible_edges"):
        return cell.visible_edges
    return "LRBT"


def _fmt_fixed(x, decimals=2):
    """Format fixe avec 'decimals' d√©cimales (ex: 0.00)."""
    try:
        if x is None:
            return ""
        x = float(x)
        if not np.isfinite(x):
            return ""
        return f"{x:.{decimals}f}"
    except Exception:
        return ""


def _apply_visual_merges_matplotlib(table, df_raw: pd.DataFrame, visual_merge):
    """
    Applique une "fusion visuelle" sur un tableau matplotlib:
    - efface le texte r√©p√©t√© (sauf premi√®re ligne du run),
    - supprime les bordures internes horizontales (T/B) dans la colonne concern√©e.
    visual_merge = list of dict:
      { "col": "<colname>", "groupby": ["colA","colB", ...] }
    """
    if not visual_merge:
        return

    df0 = df_raw.copy()

    col_index = {c: i for i, c in enumerate(df0.columns.tolist())}

    for spec in visual_merge:
        c = spec.get("col")
        groupby = spec.get("groupby", []) or []
        if c not in col_index:
            continue
        ci = col_index[c]

        # it√®re par groupes
        if groupby:
            groups = df0.groupby(groupby, sort=False)
            group_iters = [(k, g.index.to_list()) for k, g in groups]
        else:
            group_iters = [(None, df0.index.to_list())]

        for _, idxs in group_iters:
            if not idxs:
                continue
            # parcours s√©quentiel sur l'ordre actuel du DF
            run_start = idxs[0]
            prev = df0.loc[idxs[0], c]
            for ii in idxs[1:] + [None]:
                if ii is None:
                    run_end = idxs[-1]
                    same = True
                else:
                    v = df0.loc[ii, c]
                    same = (v == prev)

                if not same:
                    run_end = idxs[idxs.index(ii) - 1]

                if (ii is None) or (not same):
                    # appliquer fusion sur run [run_start, run_end] si longueur > 1
                    if run_end != run_start:
                        pos_map = {idx: p for p, idx in enumerate(df0.index.to_list())}
                        p0 = pos_map[run_start]
                        p1 = pos_map[run_end]
                        for p in range(p0, p1 + 1):
                            tr = p + 1
                            tc = ci
                            cell = table[(tr, tc)]
                            # effacer texte sauf premi√®re ligne
                            if p != p0:
                                cell.get_text().set_text("")
                            # supprimer bordures internes T/B
                            edges = set(_cell_get_visible_edges(cell))
                            edges.add("L"); edges.add("R")
                            if p != p0:
                                edges.discard("T")
                            if p != p1:
                                edges.discard("B")
                            _cell_set_visible_edges(cell, "".join([e for e in "LRBT" if e in edges]))

                    # reset run
                    if ii is not None:
                        run_start = ii
                        prev = df0.loc[ii, c]


def _save_table_png_generic(df: pd.DataFrame, out_png: Path, font_size=10, col_widths=None, visual_merge=None, decimals=2):
    """
    PNG sans titre, CROP EXACT au cadre du tableau (pas de blanc inutile),
    + fusion visuelle optionnelle via visual_merge.
    Format num√©rique : 'decimals' d√©cimales fixes (par d√©faut 2).
    """
    PAD_PX = 0

    d = df.copy()

    # format num√©rique
    for c in ['‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'z']:
        if c in d.columns:
            d[c] = pd.to_numeric(d[c], errors='coerce').map(lambda x: _fmt_fixed(x, decimals=decimals))
    if 'N' in d.columns:
        d['N'] = pd.to_numeric(d['N'], errors='coerce').map(lambda x: "" if not np.isfinite(x) else f"{int(x)}")

    fig_h = max(4.5, 0.38 * (len(d) + 1))
    fig_w = 14

    fig, ax = plt.subplots(figsize=(fig_w, fig_h))
    ax.axis('off')
    ax.set_position([0, 0, 1, 1])

    table = ax.table(
        cellText=d.values.tolist(),
        colLabels=d.columns.tolist(),
        cellLoc='center',
        colLoc='center',
        loc='upper left'
    )
    table.auto_set_font_size(False)
    table.set_fontsize(font_size)

    for (r, c), cell in table.get_celld().items():
        cell.set_linewidth(0.8)
        if r == 0:
            cell.set_facecolor('#F2F2F2')
            cell.set_text_props(weight='bold')
            cell.set_height(0.06)
        else:
            cell.set_height(0.045)

    if col_widths is not None:
        for c in range(len(col_widths)):
            for r in range(len(d) + 1):
                table[(r, c)].set_width(col_widths[c])

    _apply_visual_merges_matplotlib(table, df_raw=df, visual_merge=visual_merge)

    fig.canvas.draw()
    renderer = fig.canvas.get_renderer()
    bbox = table.get_window_extent(renderer=renderer)

    if PAD_PX and PAD_PX > 0:
        bbox = Bbox.from_extents(
            bbox.x0 - PAD_PX, bbox.y0 - PAD_PX,
            bbox.x1 + PAD_PX, bbox.y1 + PAD_PX
        )

    bbox_inches = bbox.transformed(fig.dpi_scale_trans.inverted())

    fig.savefig(out_png, dpi=300, bbox_inches=bbox_inches, pad_inches=0, facecolor='white')
    plt.close(fig)


def _save_table1_like_png(df_table: pd.DataFrame, out_png: Path, font_size=10, decimals=2):
    """
    PNG Table1 / TableM : fusion visuelle Mer + P√©riode (bords internes supprim√©s).
    """
    df2 = df_table[['Mer', 'P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', 'N']].copy()

    _save_table_png_generic(
        df2,
        out_png,
        font_size=font_size,
        col_widths=[0.18, 0.16, 0.14, 0.12, 0.12, 0.12, 0.08],
        visual_merge=[
            {"col": "Mer", "groupby": []},
            {"col": "P√©riode", "groupby": ["Mer"]},
        ],
        decimals=decimals
    )


def _save_tableD_png(df_tableD: pd.DataFrame, out_png: Path, font_size=10, decimals=2):
    """
    PNG TableD : fusion visuelle Mer (blocs par mer).
    """
    df2 = df_tableD[['Mer', 'D√©cennie', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', 'N']].copy()
    _save_table_png_generic(
        df2,
        out_png,
        font_size=font_size,
        col_widths=[0.22, 0.16, 0.14, 0.14, 0.08],
        visual_merge=[
            {"col": "Mer", "groupby": []},
        ],
        decimals=decimals
    )


# =============================================================================
# AJOUT : PNG LAT√óDECADE (version agr√©g√©e uniquement)
# =============================================================================

def _save_table_latband_decade_png(df_latdec: pd.DataFrame, out_png: Path, font_size=10, decimals=2):
    df2 = df_latdec[['Bande de latitude', 'D√©cennie', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', 'N']].copy()
    _save_table_png_generic(
        df2,
        out_png,
        font_size=font_size,
        col_widths=[0.22, 0.18, 0.14, 0.14, 0.08],
        visual_merge=[
            {"col": "Bande de latitude", "groupby": []},
        ],
        decimals=decimals
    )


# =============================================================================
# AJOUT : PNG LAT√óPERIODE√óPROFONDEUR (nouveau tableau)
# =============================================================================

def _save_table_latband_period_depth_png(df_latpd: pd.DataFrame, out_png: Path, font_size=10, decimals=2):
    df2 = df_latpd[['Bande de latitude', 'P√©riode', 'Profondeur', 'N', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)']].copy()
    _save_table_png_generic(
        df2,
        out_png,
        font_size=font_size,
        col_widths=[0.22, 0.18, 0.14, 0.08, 0.14, 0.14],
        visual_merge=[
            {"col": "Bande de latitude", "groupby": []},
            {"col": "P√©riode", "groupby": ["Bande de latitude"]},
        ],
        decimals=decimals
    )


# =============================================================================
# AJOUTS : "CASE TABLES" (3 tableaux cibl√©s) + z-score leave-one-year-out pooled
# =============================================================================

def _loo_zscore_pooled(df: pd.DataFrame, year: int, delta_col='delta_T', year_col='hist_year'):
    """
    z-score leave-one-year-out (pooled sur les POINTS des autres ann√©es)
    z = (mean_year - mean_others) / std_others
    - mean_year : moyenne des points ŒîT de l'ann√©e 'year'
    - mean_others/std_others : stats sur les points ŒîT de toutes les autres ann√©es
    """
    sub_y = df[df[year_col] == year][delta_col].to_numpy(dtype=float)
    sub_y = sub_y[np.isfinite(sub_y)]
    if sub_y.size == 0:
        return np.nan

    others = df[df[year_col] != year][delta_col].to_numpy(dtype=float)
    others = others[np.isfinite(others)]
    if others.size < 2:
        return np.nan

    mean_y = float(np.mean(sub_y))
    mean_o = float(np.mean(others))
    std_o = float(np.std(others, ddof=1)) if others.size >= 2 else np.nan
    if (not np.isfinite(std_o)) or std_o <= 0:
        return np.nan
    return float((mean_y - mean_o) / std_o)


def _build_case_table_depth_years_with_z(
    obs_pts: pd.DataFrame,
    sea_name: str,
    y0: int,
    y1_inclusive: int,
    depth_bands,
):
    """
    Construit un tableau "Profondeur / Ann√©e / N / ‚ü®ŒîT‚ü© / œÉ(ŒîT) / z"
    - z calcul√© avec leave-one-year-out pooled sur les points des autres ann√©es (r√©sout Ross).
    depth_bands = list of tuples: (d0, d1, label_str)
    """
    sea_conf = SEAS[sea_name]
    df = filter_by_sea(obs_pts, sea_conf)
    df = df[(df['hist_year'] >= y0) & (df['hist_year'] <= y1_inclusive)].copy()

    rows = []
    for (d0, d1, dlab) in depth_bands:
        sub = df[(df['hist_depth_m'] >= d0) & (df['hist_depth_m'] <= d1)].copy()
        if sub.empty:
            continue

        years = sorted(sub['hist_year'].dropna().unique().tolist())
        for yy in years:
            sy = sub[sub['hist_year'] == yy]['delta_T'].to_numpy(dtype=float)
            sy = sy[np.isfinite(sy)]
            if sy.size == 0:
                continue
            n = int(sy.size)
            m = float(np.mean(sy))
            sd = float(np.std(sy, ddof=1)) if sy.size >= 2 else np.nan
            z = _loo_zscore_pooled(sub, int(yy), delta_col='delta_T', year_col='hist_year')
            rows.append({
                'Profondeur': dlab,
                'Ann√©e': int(yy),
                'N': n,
                '‚ü®ŒîT‚ü©': m,
                'œÉ(ŒîT)': sd,
                'z': z
            })

    out = pd.DataFrame(rows)
    if out.empty:
        return out
    out = out.sort_values(['Profondeur', 'Ann√©e']).reset_index(drop=True)
    return out


def _build_case_table_years_simple(
    obs_pts: pd.DataFrame,
    sea_name: str,
    y0: int,
    y1_inclusive: int,
    depth_band=(10, 200),
):
    """
    Tableau simple "Ann√©e / N / ‚ü®ŒîT‚ü© / œÉ(ŒîT)" (ex: Weddell 1937‚Äì1957).
    """
    sea_conf = SEAS[sea_name]
    df = filter_by_sea(obs_pts, sea_conf)
    df = df[(df['hist_year'] >= y0) & (df['hist_year'] <= y1_inclusive)].copy()
    d0, d1 = depth_band
    df = df[(df['hist_depth_m'] >= d0) & (df['hist_depth_m'] <= d1)].copy()

    rows = []
    for yy in sorted(df['hist_year'].dropna().unique().tolist()):
        sy = df[df['hist_year'] == yy]['delta_T'].to_numpy(dtype=float)
        sy = sy[np.isfinite(sy)]
        if sy.size == 0:
            continue
        rows.append({
            'Ann√©e': int(yy),
            'N': int(sy.size),
            '‚ü®ŒîT‚ü©': float(np.mean(sy)),
            'œÉ(ŒîT)': float(np.std(sy, ddof=1)) if sy.size >= 2 else np.nan
        })

    out = pd.DataFrame(rows)
    if out.empty:
        return out
    out = out.sort_values(['Ann√©e']).reset_index(drop=True)
    return out


def _merge_cells_for_case_depth(ws: Worksheet, depth_col=1, start_row=2):
    """
    Fusionne la colonne 'Profondeur' pour les case tables (blocs par profondeur).
    """
    max_row = ws.max_row
    r = start_row
    while r <= max_row:
        v = ws.cell(row=r, column=depth_col).value
        if v is None:
            r += 1
            continue
        r2 = r
        while r2 <= max_row and ws.cell(row=r2, column=depth_col).value == v:
            r2 += 1
        end = r2 - 1
        if end > r:
            ws.merge_cells(start_row=r, start_column=depth_col, end_row=end, end_column=depth_col)
        r = end + 1


def _save_case_depth_table_png(df_case: pd.DataFrame, out_png: Path, font_size=12, decimals=2):
    """
    PNG case table avec fusion visuelle de 'Profondeur'.
    """
    _save_table_png_generic(
        df_case,
        out_png,
        font_size=font_size,
        col_widths=[0.22, 0.14, 0.10, 0.14, 0.14, 0.10],
        visual_merge=[{"col": "Profondeur", "groupby": []}],
        decimals=decimals
    )


def _save_case_year_table_png(df_case: pd.DataFrame, out_png: Path, font_size=12, decimals=2):
    """
    PNG case table simple (Weddell).
    """
    _save_table_png_generic(
        df_case,
        out_png,
        font_size=font_size,
        col_widths=[0.18, 0.14, 0.18, 0.18],
        visual_merge=None,
        decimals=decimals
    )


# =============================================================================
# G√âN√âRATION TABLES (Excel + PNG)
# =============================================================================

def _generate_stats_outputs(df_yam_filtered: pd.DataFrame,
                            df_nem_filtered: pd.DataFrame,
                            out_dir: Path,
                            season: str,
                            depth_min: int,
                            depth_max: int,
                            y0: int,
                            y1: int,
                            make_png=True,
                            enable_scatter_tables=True):
    """
    Produit:
    - Excel: Table1/M/D pour OBS et NEMO (1 fichier)
      + une version brute de chaque feuille: suffixe "_brute"
    - PNG: Table1 + TableM + TableD pour OBS et NEMO (si make_png)
      + Scatter SO (OBS / NEMO / Combined) avec fusion visuelle
    - AJOUT: Scatter SO "shallow bands" 10‚Äì50 & 50‚Äì100 (2 lignes par p√©riode)
    - AJOUT: LAT√óDECADE (par bande de latitude, par d√©cennie) ‚Äî OBS/NEMO + _brute
      + PNG pour la version agr√©g√©e uniquement
    - AJOUT: LAT√óPERIODE√óPROFONDEUR (par bande de latitude) ‚Äî OBS/NEMO + _brute   <-- NOUVEAU
      + PNG pour la version agr√©g√©e uniquement                                   <-- NOUVEAU
    - AJOUT: 3 "Case tables" (BA, Weddell, Ross) + z-score pooled (r√©sout Ross vide)
    - Format PNG : 2 d√©cimales fixes pour toutes les valeurs num√©riques.
    """
    PNG_DECIMALS = 2

    obs_pts = _compute_obs_delta_points(df_yam_filtered)
    nemo_pts = _compute_nemo_delta_points(df_nem_filtered)

    # Tables par mer
    t1_obs = _build_table1("OBS", obs_pts)
    t1_nem = _build_table1("NEMO", nemo_pts)

    tm_obs = _build_tableM("OBS", obs_pts)
    tm_nem = _build_tableM("NEMO", nemo_pts)

    td_obs = _build_tableD("OBS", obs_pts)
    td_nem = _build_tableD("NEMO", nemo_pts)

    # =============================================================================
    # LAT√óDECADE (OBS/NEMO)
    # =============================================================================
    latdec_obs = _build_table_latband_decade("OBS", obs_pts)
    latdec_nem = _build_table_latband_decade("NEMO", nemo_pts)

    # =============================================================================
    # NOUVEAU : LAT√óPERIODE√óPROFONDEUR (OBS/NEMO)
    # =============================================================================
    latpd_obs = _build_table_latband_period_depth("OBS", obs_pts)
    latpd_nem = _build_table_latband_period_depth("NEMO", nemo_pts)

    wb = Workbook()
    if "Sheet" in wb.sheetnames:
        del wb["Sheet"]

    ws1 = _write_df_to_sheet(wb, "Table1_OBS", t1_obs)
    _merge_cells_for_table1(ws1, sea_col=1, period_col=2, start_row=2)
    _write_df_to_sheet(wb, "Table1_OBS_brute", t1_obs)

    ws2 = _write_df_to_sheet(wb, "Table1_NEMO", t1_nem)
    _merge_cells_for_table1(ws2, sea_col=1, period_col=2, start_row=2)
    _write_df_to_sheet(wb, "Table1_NEMO_brute", t1_nem)

    wsm1 = _write_df_to_sheet(wb, "TableM_OBS", tm_obs)
    _merge_cells_for_table1(wsm1, sea_col=1, period_col=2, start_row=2)
    _write_df_to_sheet(wb, "TableM_OBS_brute", tm_obs)

    wsm2 = _write_df_to_sheet(wb, "TableM_NEMO", tm_nem)
    _merge_cells_for_table1(wsm2, sea_col=1, period_col=2, start_row=2)
    _write_df_to_sheet(wb, "TableM_NEMO_brute", tm_nem)

    wsd1 = _write_df_to_sheet(wb, "TableD_OBS", td_obs)
    _merge_cells_for_tableD(wsd1, sea_col=1, start_row=2)
    _write_df_to_sheet(wb, "TableD_OBS_brute", td_obs)

    wsd2 = _write_df_to_sheet(wb, "TableD_NEMO", td_nem)
    _merge_cells_for_tableD(wsd2, sea_col=1, start_row=2)
    _write_df_to_sheet(wb, "TableD_NEMO_brute", td_nem)

    # =============================================================================
    # LAT√óDECADE ‚Äî 4 feuilles
    # =============================================================================
    wldo = _write_df_to_sheet(wb, "LatDec_OBS", latdec_obs)
    _merge_cells_for_latband_decade(wldo, band_col=1, start_row=2)
    _write_df_to_sheet(wb, "LatDec_OBS_brute", latdec_obs)

    wldn = _write_df_to_sheet(wb, "LatDec_NEMO", latdec_nem)
    _merge_cells_for_latband_decade(wldn, band_col=1, start_row=2)
    _write_df_to_sheet(wb, "LatDec_NEMO_brute", latdec_nem)

    # =============================================================================
    # NOUVEAU : LAT√óPERIODE√óPROFONDEUR ‚Äî 4 feuilles
    # =============================================================================
    wlpdo = _write_df_to_sheet(wb, "LatPeriodDepth_OBS", latpd_obs)
    _merge_cells_for_latband_period_depth(wlpdo, band_col=1, period_col=2, start_row=2)
    _write_df_to_sheet(wb, "LatPeriodDepth_OBS_brute", latpd_obs)

    wlpdn = _write_df_to_sheet(wb, "LatPeriodDepth_NEMO", latpd_nem)
    _merge_cells_for_latband_period_depth(wlpdn, band_col=1, period_col=2, start_row=2)
    _write_df_to_sheet(wb, "LatPeriodDepth_NEMO_brute", latpd_nem)

    # --- Scatter SO (propre + brute) + AJOUT shallow
    if enable_scatter_tables:
        # Scatter standard (inchang√©)
        scatter_obs = _build_scatter_SO_table("OBS", obs_pts)
        scatter_nem = _build_scatter_SO_table("NEMO", nemo_pts)
        scatter_combined = pd.concat([scatter_obs, scatter_nem], ignore_index=True)
        scatter_combined = scatter_combined[['Dataset', 'P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']]

        wso = _write_df_to_sheet(
            wb, "Scatter_SO_OBS",
            scatter_obs[['P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']]
        )
        _merge_cells_for_scatter_period(wso, period_col=1, start_row=2)
        _write_df_to_sheet(
            wb, "Scatter_SO_OBS_brute",
            scatter_obs[['P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']]
        )

        wsn = _write_df_to_sheet(
            wb, "Scatter_SO_NEMO",
            scatter_nem[['P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']]
        )
        _merge_cells_for_scatter_period(wsn, period_col=1, start_row=2)
        _write_df_to_sheet(
            wb, "Scatter_SO_NEMO_brute",
            scatter_nem[['P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']]
        )

        wsc = _write_df_to_sheet(wb, "Scatter_SO_Combined", scatter_combined)
        _merge_cells_for_scatter_combined(wsc, dataset_col=1, period_col=2, start_row=2)
        _write_df_to_sheet(wb, "Scatter_SO_Combined_brute", scatter_combined)

        # ----------------------------
        # AJOUT: Scatter shallow 10‚Äì50 et 50‚Äì100 (2 lignes par p√©riode)
        # ----------------------------
        shallow_depths = [(10, 50), (50, 100)]
        scatter_obs_sh = _build_scatter_SO_table("OBS", obs_pts, depths=shallow_depths)
        scatter_nem_sh = _build_scatter_SO_table("NEMO", nemo_pts, depths=shallow_depths)
        scatter_combined_sh = pd.concat([scatter_obs_sh, scatter_nem_sh], ignore_index=True)
        scatter_combined_sh = scatter_combined_sh[['Dataset', 'P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']]

        # OBS shallow
        wso2 = _write_df_to_sheet(
            wb, "Scatter_SO_OBS_10-50_50-100",
            scatter_obs_sh[['P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']]
        )
        _merge_cells_for_scatter_period(wso2, period_col=1, start_row=2)
        _write_df_to_sheet(
            wb, "Scatter_SO_OBS_10-50_50-100_brute",
            scatter_obs_sh[['P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']]
        )

        # NEMO shallow
        wsn2 = _write_df_to_sheet(
            wb, "Scatter_SO_NEMO_10-50_50-100",
            scatter_nem_sh[['P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']]
        )
        _merge_cells_for_scatter_period(wsn2, period_col=1, start_row=2)
        _write_df_to_sheet(
            wb, "Scatter_SO_NEMO_10-50_50-100_brute",
            scatter_nem_sh[['P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']]
        )

        # Combined shallow
        wsc2 = _write_df_to_sheet(wb, "Scatter_SO_Combined_10-50_50-100", scatter_combined_sh)
        _merge_cells_for_scatter_combined(wsc2, dataset_col=1, period_col=2, start_row=2)
        _write_df_to_sheet(wb, "Scatter_SO_Combined_10-50_50-100_brute", scatter_combined_sh)

    # =============================================================================
    # AJOUT : 3 CASE TABLES (OBS) ‚Äî BA / Weddell / Ross
    # =============================================================================
    # 1) BA 1930‚Äì1939 : 10‚Äì200 + 130‚Äì160 avec z
    case_ba = _build_case_table_depth_years_with_z(
        obs_pts=obs_pts,
        sea_name='Bellingshausen-Amundsen',
        y0=1930,
        y1_inclusive=1939,
        depth_bands=[
            (10, 200, "10‚Äì200 m"),
            (130, 160, "130‚Äì160 m"),
        ],
    )

    # 2) Weddell 1937‚Äì1957 : 10‚Äì200 (simple)
    case_wed = _build_case_table_years_simple(
        obs_pts=obs_pts,
        sea_name='Weddell',
        y0=1937,
        y1_inclusive=1957,
        depth_band=(10, 200),
    )

    # 3) Ross 1920‚Äì1929 : 10‚Äì200 + 80‚Äì110 avec z (FIX z pooled => plus de colonne vide)
    case_ross = _build_case_table_depth_years_with_z(
        obs_pts=obs_pts,
        sea_name='Ross',
        y0=1920,
        y1_inclusive=1929,
        depth_bands=[
            (10, 200, "10‚Äì200 m"),
            (80, 110, "80‚Äì110 m"),
        ],
    )

    # --- Excel sheets
    if not case_ba.empty:
        ws_ba = _write_df_to_sheet(wb, "Case_BA_1930-1939", case_ba)
        _merge_cells_for_case_depth(ws_ba, depth_col=1, start_row=2)
        _write_df_to_sheet(wb, "Case_BA_1930-1939_brute", case_ba)

    if not case_wed.empty:
        _write_df_to_sheet(wb, "Case_Weddell_1937-1957", case_wed)
        _write_df_to_sheet(wb, "Case_Weddell_1937-1957_brute", case_wed)

    if not case_ross.empty:
        ws_ro = _write_df_to_sheet(wb, "Case_Ross_1920-1929", case_ross)
        _merge_cells_for_case_depth(ws_ro, depth_col=1, start_row=2)
        _write_df_to_sheet(wb, "Case_Ross_1920-1929_brute", case_ross)

    xlsx_name = f"StatsTables_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.xlsx"
    out_xlsx = out_dir / xlsx_name
    wb.save(out_xlsx)
    print(f"\n‚úÖ Excel stats g√©n√©r√© : {out_xlsx.name}")

    # =============================================================================
    # PNG : Table1 + TableM + TableD (OBS/NEMO) + Scatter SO + LAT√óDECADE + LAT√óPERIODE√óPROFONDEUR + CASES
    # =============================================================================
    if make_png:
        # ---- Table1
        out_png_t1_obs = out_dir / f"Table1_OBS_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
        out_png_t1_nem = out_dir / f"Table1_NEMO_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
        _save_table1_like_png(t1_obs, out_png_t1_obs, font_size=10, decimals=PNG_DECIMALS)
        _save_table1_like_png(t1_nem, out_png_t1_nem, font_size=10, decimals=PNG_DECIMALS)
        print(f"‚úÖ PNG Table1 OBS : {out_png_t1_obs.name}")
        print(f"‚úÖ PNG Table1 NEMO: {out_png_t1_nem.name}")

        # ---- TableM
        out_png_tm_obs = out_dir / f"TableM_OBS_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
        out_png_tm_nem = out_dir / f"TableM_NEMO_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
        _save_table1_like_png(tm_obs, out_png_tm_obs, font_size=13, decimals=PNG_DECIMALS)
        _save_table1_like_png(tm_nem, out_png_tm_nem, font_size=13, decimals=PNG_DECIMALS)
        print(f"‚úÖ PNG TableM OBS : {out_png_tm_obs.name}")
        print(f"‚úÖ PNG TableM NEMO: {out_png_tm_nem.name}")

        # ---- TableD
        out_png_td_obs = out_dir / f"TableD_OBS_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
        out_png_td_nem = out_dir / f"TableD_NEMO_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
        _save_tableD_png(td_obs, out_png_td_obs, font_size=10, decimals=PNG_DECIMALS)
        _save_tableD_png(td_nem, out_png_td_nem, font_size=10, decimals=PNG_DECIMALS)
        print(f"‚úÖ PNG TableD OBS : {out_png_td_obs.name}")
        print(f"‚úÖ PNG TableD NEMO: {out_png_td_nem.name}")

        # ---- LAT√óDECADE (OBS/NEMO)
        out_png_lat_obs = out_dir / f"LatDec_OBS_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
        out_png_lat_nem = out_dir / f"LatDec_NEMO_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
        _save_table_latband_decade_png(latdec_obs, out_png_lat_obs, font_size=10, decimals=PNG_DECIMALS)
        _save_table_latband_decade_png(latdec_nem, out_png_lat_nem, font_size=10, decimals=PNG_DECIMALS)
        print(f"‚úÖ PNG LatDec OBS : {out_png_lat_obs.name}")
        print(f"‚úÖ PNG LatDec NEMO: {out_png_lat_nem.name}")

        # ---- NOUVEAU : LAT√óPERIODE√óPROFONDEUR (OBS/NEMO)
        out_png_latpd_obs = out_dir / f"LatPeriodDepth_OBS_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
        out_png_latpd_nem = out_dir / f"LatPeriodDepth_NEMO_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
        _save_table_latband_period_depth_png(latpd_obs, out_png_latpd_obs, font_size=10, decimals=PNG_DECIMALS)
        _save_table_latband_period_depth_png(latpd_nem, out_png_latpd_nem, font_size=10, decimals=PNG_DECIMALS)
        print(f"‚úÖ PNG LatPeriodDepth OBS : {out_png_latpd_obs.name}")
        print(f"‚úÖ PNG LatPeriodDepth NEMO: {out_png_latpd_nem.name}")

        # ---- Scatter SO (standard + shallow)
        if enable_scatter_tables:
            scatter_obs = _build_scatter_SO_table("OBS", obs_pts)
            scatter_nem = _build_scatter_SO_table("NEMO", nemo_pts)
            scatter_combined = pd.concat([scatter_obs, scatter_nem], ignore_index=True)
            scatter_combined = scatter_combined[['Dataset', 'P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']]

            out_png_sc_obs = out_dir / f"ScatterSO_OBS_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
            out_png_sc_nem = out_dir / f"ScatterSO_NEMO_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
            out_png_sc_com = out_dir / f"ScatterSO_Combined_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"

            _save_table_png_generic(
                scatter_obs[['P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']],
                out_png_sc_obs,
                font_size=10,
                col_widths=[0.18, 0.14, 0.12, 0.12, 0.14, 0.14, 0.08],
                visual_merge=[{"col": "P√©riode", "groupby": []}],
                decimals=PNG_DECIMALS
            )

            _save_table_png_generic(
                scatter_nem[['P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']],
                out_png_sc_nem,
                font_size=10,
                col_widths=[0.18, 0.14, 0.12, 0.12, 0.14, 0.14, 0.08],
                visual_merge=[{"col": "P√©riode", "groupby": []}],
                decimals=PNG_DECIMALS
            )

            _save_table_png_generic(
                scatter_combined,
                out_png_sc_com,
                font_size=10,
                col_widths=[0.12, 0.18, 0.14, 0.12, 0.12, 0.14, 0.14, 0.08],
                visual_merge=[
                    {"col": "Dataset", "groupby": []},
                    {"col": "P√©riode", "groupby": ["Dataset"]},
                ],
                decimals=PNG_DECIMALS
            )

            print(f"‚úÖ PNG Scatter SO OBS     : {out_png_sc_obs.name}")
            print(f"‚úÖ PNG Scatter SO NEMO    : {out_png_sc_nem.name}")
            print(f"‚úÖ PNG Scatter SO Combined: {out_png_sc_com.name}")

            # --- AJOUT PNG shallow (10‚Äì50 / 50‚Äì100)
            shallow_depths = [(10, 50), (50, 100)]
            scatter_obs_sh = _build_scatter_SO_table("OBS", obs_pts, depths=shallow_depths)
            scatter_nem_sh = _build_scatter_SO_table("NEMO", nemo_pts, depths=shallow_depths)
            scatter_combined_sh = pd.concat([scatter_obs_sh, scatter_nem_sh], ignore_index=True)
            scatter_combined_sh = scatter_combined_sh[['Dataset', 'P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']]

            out_png_sc_obs_sh = out_dir / f"ScatterSO_OBS_10-50_50-100_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
            out_png_sc_nem_sh = out_dir / f"ScatterSO_NEMO_10-50_50-100_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
            out_png_sc_com_sh = out_dir / f"ScatterSO_Combined_10-50_50-100_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"

            _save_table_png_generic(
                scatter_obs_sh[['P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']],
                out_png_sc_obs_sh,
                font_size=10,
                col_widths=[0.18, 0.14, 0.12, 0.12, 0.14, 0.14, 0.08],
                visual_merge=[{"col": "P√©riode", "groupby": []}],
                decimals=PNG_DECIMALS
            )

            _save_table_png_generic(
                scatter_nem_sh[['P√©riode', 'Profondeur', '‚ü®ŒîT‚ü©', 'œÉ(ŒîT)', '% ŒîT < 0', '% ŒîT > 0', 'N']],
                out_png_sc_nem_sh,
                font_size=10,
                col_widths=[0.18, 0.14, 0.12, 0.12, 0.14, 0.14, 0.08],
                visual_merge=[{"col": "P√©riode", "groupby": []}],
                decimals=PNG_DECIMALS
            )

            _save_table_png_generic(
                scatter_combined_sh,
                out_png_sc_com_sh,
                font_size=10,
                col_widths=[0.12, 0.18, 0.14, 0.12, 0.12, 0.14, 0.14, 0.08],
                visual_merge=[
                    {"col": "Dataset", "groupby": []},
                    {"col": "P√©riode", "groupby": ["Dataset"]},
                ],
                decimals=PNG_DECIMALS
            )

            print(f"‚úÖ PNG Scatter SO OBS (10‚Äì50/50‚Äì100)     : {out_png_sc_obs_sh.name}")
            print(f"‚úÖ PNG Scatter SO NEMO (10‚Äì50/50‚Äì100)    : {out_png_sc_nem_sh.name}")
            print(f"‚úÖ PNG Scatter SO Combined (10‚Äì50/50‚Äì100): {out_png_sc_com_sh.name}")

        # ---- AJOUT PNG CASES
        if not case_ba.empty:
            out_png_case_ba = out_dir / f"Case_BA_1930-1939_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
            _save_case_depth_table_png(case_ba, out_png_case_ba, font_size=12, decimals=PNG_DECIMALS)
            print(f"‚úÖ PNG Case BA : {out_png_case_ba.name}")

        if not case_wed.empty:
            out_png_case_wed = out_dir / f"Case_Weddell_1937-1957_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
            _save_case_year_table_png(case_wed, out_png_case_wed, font_size=12, decimals=PNG_DECIMALS)
            print(f"‚úÖ PNG Case Weddell : {out_png_case_wed.name}")

        if not case_ross.empty:
            out_png_case_ross = out_dir / f"Case_Ross_1920-1929_{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}.png"
            _save_case_depth_table_png(case_ross, out_png_case_ross, font_size=12, decimals=PNG_DECIMALS)
            print(f"‚úÖ PNG Case Ross : {out_png_case_ross.name}")


# =============================================================================
# PANNEAU PRINCIPAL
# =============================================================================

def create_combined_panel(df_yamazaki, df_nemo, out_dir, season,
                          depth_min, depth_max, y0, y1,
                          enable_forensics=False,
                          enable_vertical_stats=False,
                          enable_new_tables=True,
                          tables_make_png=True,
                          enable_scatter_tables=True):
    """Cr√©e le panneau combin√© 4√ó2 avec logique 0-360¬∞."""
    print(f"\nüìä G√©n√©ration du panneau combin√© 4√ó2...")

    fig = plt.figure(figsize=(16, 18))
    gs = GridSpec(4, 2, figure=fig,
                  left=0.08, right=0.96,
                  top=0.97, bottom=0.06,
                  hspace=0.18, wspace=0.12)

    axes_yamazaki = []
    axes_nemo = []

    for i in range(4):
        axes_yamazaki.append(fig.add_subplot(gs[i, 0]))
        axes_nemo.append(fig.add_subplot(gs[i, 1]))

    season_months = SEASON_MONTHS[season]

    # Filtrage initial (base de TOUT ce qu‚Äôon sort en stats)
    df_yam = df_yamazaki[
        (df_yamazaki['hist_month'].isin(season_months)) &
        (df_yamazaki['hist_depth_m'] >= depth_min) &
        (df_yamazaki['hist_depth_m'] <= depth_max) &
        (df_yamazaki['hist_year'] >= y0) &
        (df_yamazaki['hist_year'] < y1)
    ].copy()

    df_nem = df_nemo[
        (df_nemo['hist_month'].isin(season_months)) &
        (df_nemo['hist_depth_m'] >= depth_min) &
        (df_nemo['hist_depth_m'] <= depth_max) &
        (df_nemo['hist_year'] >= y0) &
        (df_nemo['hist_year'] < y1)
    ].copy()

    print(f"   Yamazaki: {len(df_yam):,} observations")
    print(f"   NEMO: {len(df_nem):,} observations")

    # === TABLEAUX (Excel + PNG) ===
    if enable_new_tables:
        _generate_stats_outputs(
            df_yam_filtered=df_yam,
            df_nem_filtered=df_nem,
            out_dir=out_dir,
            season=season,
            depth_min=depth_min,
            depth_max=depth_max,
            y0=y0,
            y1=y1,
            make_png=tables_make_png,
            enable_scatter_tables=enable_scatter_tables
        )

    stats_rows = []  # vertical stats (optionnel)

    # Tracer chaque mer
    for i, sea_name in enumerate(SEA_ORDER):
        sea_config = SEAS[sea_name]

        df_sea_yam = filter_by_sea(df_yam, sea_config)
        df_sea_nem = filter_by_sea(df_nem, sea_config)

        print(f"\nüîç {sea_name}:")
        print(f"   Yamazaki: {len(df_sea_yam):,} points")
        if len(df_sea_yam) > 0:
            print(f"      Lon range: {df_sea_yam['hist_lon'].min():.1f}¬∞ to "
                  f"{df_sea_yam['hist_lon'].max():.1f}¬∞ (format -180/180)")
            print(f"      Lon 0-360: {df_sea_yam['lon_360'].min():.1f}¬∞ to "
                  f"{df_sea_yam['lon_360'].max():.1f}¬∞")

        print(f"   NEMO: {len(df_sea_nem):,} points")
        if len(df_sea_nem) > 0:
            print(f"      Lon range: {df_sea_nem['hist_lon'].min():.1f}¬∞ to "
                  f"{df_sea_nem['hist_lon'].max():.1f}¬∞ (format -180/180)")
            print(f"      Lon 0-360: {df_sea_nem['lon_360'].min():.1f}¬∞ to "
                  f"{df_sea_nem['lon_360'].max():.1f}¬∞")

        _plot_yamazaki_sea_on_ax(axes_yamazaki[i], df_sea_yam, sea_name, season)
        _plot_nemo_sea_on_ax(axes_nemo[i], df_sea_nem, sea_name, season)

        sea_label = SEAS[sea_name]['label']
        axes_yamazaki[i].text(-0.12, 0.5, sea_label,
                              transform=axes_yamazaki[i].transAxes,
                              ha='right', va='center',
                              fontsize=14, fontweight='bold')

        axes_yamazaki[-1].set_xlabel('Anomalie de temp√©rature (¬∞C)', fontsize=13)
        axes_nemo[-1].set_xlabel('Anomalie de temp√©rature (¬∞C)', fontsize=13)

        axes_yamazaki[i].set_ylabel('Profondeur (m)', fontsize=13)

        if enable_vertical_stats:
            stats_row = _compute_vertical_stats_for_sea(
                sea_name,
                df_sea_yam,
                df_sea_nem,
                depth_min=depth_min,
                depth_max=depth_max
            )
            stats_rows.append(stats_row)

        if enable_forensics:
            _compute_profile_forensics_tables(
                sea_name=sea_name,
                df_sea_yam=df_sea_yam,
                df_sea_nem=df_sea_nem,
                out_dir=out_dir,
                depth_min=depth_min,
                depth_max=depth_max,
                bin_size=10,
                cell_res_deg=2,
                do_bootstrap=True,
                silent=False
            )

    fig.text(0.29, 0.985, 'Observations historiques',
             ha='center', va='top',
             fontsize=15, fontweight='bold')

    fig.text(0.74, 0.985, 'Simulation NEMO',
             ha='center', va='top',
             fontsize=15, fontweight='bold')

    legend_elements = [
        Line2D([0], [0], color=DECADE_COLORS[i], linewidth=2,
               marker='o', markersize=4,
               label=f'{dec0}-{dec1-1}')
        for i, (dec0, dec1) in enumerate(DECADES)
    ]

    fig.legend(handles=legend_elements,
               loc='lower center',
               bbox_to_anchor=(0.45, -0.01),
               ncol=7,
               fontsize=14,
               frameon=True,
               title='D√©cennies',
               title_fontsize=15)

    prefix = f"{season}_{depth_min}-{depth_max}m_{y0}-{y1-1}"
    out_path = out_dir / f"Combined_SeaAnomalies_{prefix}_4x2_panel.png"
    plt.savefig(out_path, dpi=200, bbox_inches='tight', facecolor='white')
    plt.close()

    print(f"\n‚úÖ Panneau combin√© sauvegard√© : {out_path.name}")

    if enable_vertical_stats:
        _save_vertical_stats_table(stats_rows, out_dir, silent=False)


# =============================================================================
# MAIN
# =============================================================================

def main():
    ap = argparse.ArgumentParser(
        description="Panneau combin√© 4x2 Yamazaki + NEMO (LON 0-360¬∞)"
    )
    ap.add_argument("--yamazaki-csv-dir", type=str, required=True)
    ap.add_argument("--nemo-csv-dir", type=str, required=True)
    ap.add_argument("--out-dir", type=str, required=True)
    ap.add_argument("--season", type=str, default="DJF")
    ap.add_argument("--depth-min", type=int, default=10)
    ap.add_argument("--depth-max", type=int, default=200)
    ap.add_argument("--hist-y0", type=int, default=1900)
    ap.add_argument("--hist-y1", type=int, default=1970)

    # === FLAGS ===
    ap.add_argument("--enable-forensics", action="store_true",
                    help="R√©active les sorties 'forensics' (CSV + prints). OFF par d√©faut.")
    ap.add_argument("--enable-vertical-stats", action="store_true",
                    help="R√©active les vertical stats legacy (CSV + prints). OFF par d√©faut.")
    ap.add_argument("--disable-new-tables", action="store_true",
                    help="D√©sactive les tableaux Excel/PNG. ON par d√©faut.")
    ap.add_argument("--tables-no-png", action="store_true",
                    help="Ne g√©n√®re pas les PNG (Excel uniquement).")
    ap.add_argument("--disable-scatter-tables", action="store_true",
                    help="D√©sactive les tableaux Scatter_SO (Excel + PNG). ON par d√©faut.")

    args = ap.parse_args()

    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 80)
    print("PANNEAU COMBIN√â 4√ó2 : Yamazaki + NEMO (LON 0-360¬∞)")
    print("=" * 80)

    print("\nüìÇ Chargement Yamazaki...")
    csv_pattern_yam = str(Path(args.yamazaki_csv_dir) /
                          "yamazaki_en4_wod_DJF_[0-9]*.csv")
    df_yamazaki = _safe_read_csvs(csv_pattern_yam)

    if df_yamazaki.empty:
        print("‚ùå Aucune donn√©e Yamazaki trouv√©e")
        return

    if 'yamazaki_T' not in df_yamazaki.columns:
        print("‚ùå Colonne 'yamazaki_T' manquante")
        return

    print(f"   ‚úÖ Yamazaki charg√©: {len(df_yamazaki):,} lignes")

    print("\nüìÇ Chargement NEMO...")
    csv_pattern_nemo = str(Path(args.nemo_csv_dir) /
                           "nemo_yamazaki_DJF_[0-9]*.csv")
    df_nemo = _safe_read_csvs(csv_pattern_nemo)

    if df_nemo.empty:
        print("‚ùå Aucune donn√©e NEMO trouv√©e")
        return

    if ('nemo_recent_T' not in df_nemo.columns or
            'nemo_hist_T' not in df_nemo.columns):
        print("‚ùå Colonnes NEMO manquantes")
        return

    print(f"   ‚úÖ NEMO charg√©: {len(df_nemo):,} lignes")

    create_combined_panel(
        df_yamazaki, df_nemo, out_dir,
        args.season, args.depth_min, args.depth_max,
        args.hist_y0, args.hist_y1,
        enable_forensics=args.enable_forensics,
        enable_vertical_stats=args.enable_vertical_stats,
        enable_new_tables=(not args.disable_new_tables),
        tables_make_png=(not args.tables_no_png),
        enable_scatter_tables=(not args.disable_scatter_tables)
    )

    print(f"\n{'=' * 80}\n‚úÖ TERMIN√â\n{'=' * 80}")


if __name__ == "__main__":
    main()
